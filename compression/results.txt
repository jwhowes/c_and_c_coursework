ppmc_huffman (encoding sherlock_shorter (originally 754 KB))
	N = 2: 12 mins,		280 KB	(checked)
	N = 4: 9.39 mins,	227 KB	(checked)
	N = 5: 8.56 mins,	224 KB	(checked)
	N = 6: 9.83 mins,	225 KB	(checked)
	N = 7: 9.39 mins,	226 KB	(checked)
	N = 8: 10.65 mins,	228 KB	(checked)

dict_ppmc_huffman (dict encoding followed by ppmc_huffman) (encoding sherlock_shorter) (times are for ppmc_huffman):
	N = 5: 12.18 mins	213 KB	(checked)

dict_lzfg_ppmc_huffman:
	N = 5: 41 mins,		328 KB	(unchecked)

dict_bwt_ppmc_huffman: (times are for ppmc_huffman)
	N = 5: A very very long time,	259 KB (unchecked)

dict_bwt_lzfg_huffman: (times are for lzfg)
	10.1342545033 mins,	274 KB (unchecked)

dict_bwt_lzfg_ppmc_huffman: (times are for lzfg + ppmc_huffman):
	N = 3: 37 mins,		289 KB (unchecked) (interesting that it's worse than just huffman)

An idea for a workflow combining all steps: dict_LZW_bwt_ppmc_huffman (may require LZW to be modified to never output 255 (can be done in a similar way to dict))

With regards to Lempel-Ziv: LZW > LZFG > LZ77 > LZ78 (> in this context means better)

It appears that when combined with LZ, bwt is pretty good but when combined with PPMC it doesn't quite match up. My guess would be that the dict encoder creates such strong contextual redundancy that bwt can't surpass it.
Although bwt does have patches of repeated symbols, much of the document consists of blocks from which these repeated symbols are taken meaning they are more random and are less compressable.
For certain compression methods this still results in an improvement (e.g. LZ or RLE), but for some methods (including PPM) this actually reduces compression effectiveness.