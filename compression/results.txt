dict->ppmc (initialised):
	N = 4:	2.018 bpc

dict->ppmc (unitialised):
	N = 4:	2.073 bpc

ppmc (unitialised) (no dict encoding first):
	N = 4:	2.165 bpc
	N = 5:	2.125 bpc
	N = 6:	2.144 bpc

COMPRESSING dict_compressed:
mnp5:	5.065 bpc
lz77:	
lz78:	3.332 bpc
lzfg:	3.535 bpc
lzw:	3.204 bpc
ppmc:	2.073 bpc	(N=4)

COMPRESSING og_bwt_encoded (the encoded dict version):
mnp5:	3.305 bpc
lz77:	6.387 bpc	(need to redo, I had the wrong parameters when I was doing this)
lz78:	3.095 bpc
lzfg:	3.152 bpc
lzw:	3.175 bpc
ppmc:	2.447 bpc	(N=5)

An idea for a workflow combining all steps: dict_LZW_bwt_ppmc_huffman (may require LZW to be modified to never output 255 (can be done in a similar way to dict))

With regards to Lempel-Ziv: LZW > LZFG > LZ77 > LZ78 (> in this context means better)

It appears that when combined with LZ, bwt is pretty good but when combined with PPMC it doesn't quite match up. My guess would be that the dict encoder creates such strong contextual redundancy that bwt can't surpass it.
Although bwt does have patches of repeated symbols, much of the document consists of blocks from which these repeated symbols are taken meaning they are more random and are less compressable.
For certain compression methods this still results in an improvement (e.g. LZ or RLE), but for some methods (including PPM) this actually reduces compression effectiveness.